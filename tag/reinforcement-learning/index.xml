<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning | Yangzhe Kong</title>
    <link>https://yanko96.github.io/tag/reinforcement-learning/</link>
      <atom:link href="https://yanko96.github.io/tag/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Reinforcement Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 31 May 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yanko96.github.io/media/icon_hu57655349161825f6561971e3a26d7f39_2668_512x512_fill_lanczos_center_3.png</url>
      <title>Reinforcement Learning</title>
      <link>https://yanko96.github.io/tag/reinforcement-learning/</link>
    </image>
    
    <item>
      <title>Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation</title>
      <link>https://yanko96.github.io/project/learn_cvrptw/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      <guid>https://yanko96.github.io/project/learn_cvrptw/</guid>
      <description>&lt;p&gt;The project was done at Huawei Technologies Co., Ltd., and is vailable on Arxiv.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tang Q, Kong Y, Pan L, Lee C. Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation. arXiv preprint arXiv:2207.09860. 2022 Jul 20. &lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Paper [&lt;a href=&#34;https://arxiv.org/pdf/2207.09860&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Exact algorithms is not always computationally efficient enough to deploy, especially on large scenarios with complex constraints. Other methods like heuristics or meta-heuristics solvers are also faced with the same dilemma. Vehicle Routing Problems (VRPs) in real-world applications often come with various constraints, therefore bring additional computational challenges to classical algorithms like exact solution methods or heuristic search approaches. The recent idea to learn heuristic move patterns from sample data has become increasingly promising to reduce solution developing costs. However, using learning-based approaches to address more types of constrained VRP remains a challenge.&lt;/p&gt;
&lt;h2 id=&#34;trajectory-shaping&#34;&gt;Trajectory Shaping&lt;/h2&gt;
&lt;p&gt;We improve the model performance by intervening the trajectory generation process to boost the quality of the agent’s training information. The motivation is similar to modifying the expression of return. Due to the large search space and the sparsity of optima, guiding the agent to explore and learn the ’good’ actions can be very slow or easily trapped into local optima, especially if the initial state solution is far from the true global optimum. With the underlying model being deterministic and we can easily obtain the next state&amp;rsquo;s reward and cost, we suggest a post-action rejection rule deciding whether to reject the candidate solution respectively when non-improved and improved solutions are found to modify the generated trajectories.&lt;/p&gt;
&lt;img src=&#34;https://latex.codecogs.com/svg.image?&amp;space;&amp;space;&amp;space;&amp;space;P(\textnormal{Reject})&amp;space;=&amp;space;\left\{&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;\begin{array}{ll}&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;\phi&amp;space;&amp;&amp;space;\quad&amp;space;\textnormal{if&amp;space;improved}&amp;space;\\&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;1&amp;space;-&amp;space;\phi&amp;space;&amp;&amp;space;\quad&amp;space;\textnormal{if&amp;space;not&amp;space;improved},&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;&amp;space;\end{array}&amp;space;&amp;space;&amp;space;&amp;space;\right.&#34; title=&#34;https://latex.codecogs.com/svg.image? P(\textnormal{Reject}) = \left\{ \begin{array}{ll} \phi &amp; \quad \textnormal{if improved} \\ 1 - \phi &amp; \quad \textnormal{if not improved}, \end{array} \right.&#34; /&gt;
&lt;div align=center&gt;&lt;img src=&#34;tc_not_tc.png&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;modified-return&#34;&gt;Modified Return&lt;/h2&gt;
&lt;p&gt;The expression of $G_{t}$ is specially designed to encourage better performance in soft-constrained VRPs with 2-exchange moves. First, the immediate reward is penalized by the immediate cost such that the agent is encouraged to find better moves while balancing the reward and cost with iteratively updated $\lambda$s. In addition, We calculate the cumulative value using the maximum value of all pairs of subsequent moves from $s_{t}$ to $s_{t&amp;rsquo;}$ instead of a summation over all consecutive moves from $s_{t}$ to $s_{t+1}$ as in the $Return$ definition. &amp;ldquo;Bad&amp;rdquo; operations that do not improve the objective function will be suppressed, while only the &amp;lsquo;good&amp;rsquo; actions are rewarded with the $\max$ function. It also tends to decorrelate the effect of a series of historical operations so that the agent is less affected by locally optimal trajectories. To sum up, we apply such modification to better mimic the heuristic search process by encouraging more immediate and effective actions that improve the cost-penalized objective function. The following figure provides a visual representation of the definition of $G_t$.&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;return.png&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;div align=center&gt;&lt;img src=&#34;return_vs.png&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;performance&#34;&gt;Performance&lt;/h2&gt;
&lt;p&gt;We observed slightly better performance than Google OR-Tools and close performance to LKH-3.&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;perf.PNG&#34; width=&#34;800&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;concerns-on-the-dataset&#34;&gt;Concerns on the dataset&lt;/h2&gt;
&lt;p&gt;Although generation of VRP/CVRP datasets is pretty intuitive, VRPTW datasets are tricky to deal with. In our implementation we generate first a CVRP scenario and then a CVRP solution by heuristics. Time windows are then generated according to arrival time in the CVRP solution to make sure that there is at least one valid sulution. However, we believe that there are better ways to generate VRPTW/CVRPTW datsaets.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A2C Agent for playing Wimblepong with pretrained VAE as the encoder</title>
      <link>https://yanko96.github.io/project/a2c_pretrain/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://yanko96.github.io/project/a2c_pretrain/</guid>
      <description>&lt;p&gt;This project served as the final project of course
ELEC-E8125&amp;ndash;Reinforcement-learning D. The code is available &lt;a href=&#34;https://github.com/Yanko96/ELEC-E8125---Reinforcement-learning-D-Final-Project-Wimble-Pong&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wimblepong is a two player version of the pong-v0 OpenAI Gym environment developed by Intelligent Robotics group at Aalto University, Finland.&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;In this project, we were asked to develop an agent for wimblepong and the agents will be tested in a battle royale. In addition, we have 2 options for state space: the visual observation or the encoded vector of the state. Altough many classmates chose to clone github repos of SOTA algorithms such as TRPO, PPO and Dueling Deep Q Networks, I decided to challenge myself and verify one of my questions: Will supervised pretained models help accelerate divergence of reinforcement learning agents? Therefore, I chose to use visual observations and first train a VAE to encode the visual observation, then train an A2C agent of which the input is the encoded state from the VAE encoder.&lt;/p&gt;
&lt;p&gt;For sure A2C cannot be better than fancier algorithms, I&amp;rsquo;m still proud of myself, for bringing up ideas and verifying them independently.&lt;/p&gt;
&lt;h2 id=&#34;pretrained-cnn-vae&#34;&gt;Pretrained CNN-VAE&lt;/h2&gt;
&lt;p&gt;A CNN-VAE is pre-trained on collected observations of the wimblepong environment in order to accelerate the converge of the agent training. The VAE adopts a similar model strcture as ResNet. Some of the results on the test set are shown below.&lt;/p&gt;
&lt;img src=&#34;reconstructed_0.png&#34; width=&#34;800&#34;&gt;
&lt;img src=&#34;reconstructed_1.png&#34; width=&#34;800&#34;&gt;
&lt;img src=&#34;reconstructed_3.png&#34; width=&#34;800&#34;&gt;
&lt;img src=&#34;reconstructed_4.png&#34; width=&#34;800&#34;&gt;
&lt;h2 id=&#34;a2c-agent&#34;&gt;A2C Agent&lt;/h2&gt;
&lt;p&gt;The encoder of the Agent is loaded from the checkpoint of the encoder of the pre-trained CNN-VAE. Then the agent is trained by A2C algorithm with entropy loss to encourage exploration. With pre-trained VAE loaded as the encoder, the convergence of the agent is accelerated as the following figures show (green paddle is the agent).&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;return_vs.png&#34; width=600&gt;&lt;/div&gt;
&lt;div align=center&gt;&lt;img src=&#34;win_rate_vs.png&#34; width=600&gt;&lt;/div&gt;
&lt;div align=center&gt;&lt;img src=&#34;replay.gif&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The pretrained encoder did help accelerate the convergence. However, there are several reasons why I don&amp;rsquo;t recommend doing so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There&amp;rsquo;s a big gap between reconstructing the observations and predicting reliable actions and q-values. This makes pretrained model not completely plug-and-play for RL tasks. I spent many efforts selecting most suitable checkpoints and learning rates. It&amp;rsquo;s not so worthwhile, especially considering that it only accelerate a relatively small amount of training time, but can hardly boost the performance.&lt;/li&gt;
&lt;li&gt;The model structure of VAE is not necessarily the best for RL models.&lt;/li&gt;
&lt;li&gt;Exploration is the most crucial for RL. Not these tricks (that are not helpful for exploration).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Anyways, it&amp;rsquo;s still an interesting experience for me.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
